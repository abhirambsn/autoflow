from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv
import os, uuid, json
from typing import Dict, Optional
from .util.github_handler import GithubHandler
from .util.repo_manager import RepositoryManager
from .util.infra_generator import InfraGenerator
from redis import Redis
from rq import Queue

load_dotenv()

if "GOOGLE_API_KEY" not in os.environ:
    raise ValueError("GOOGLE_API_KEY environment variable is not set.")
if "GITHUB_APP_TOKEN" not in os.environ:
    raise ValueError("GITHUB_APP_TOKEN environment variable is not set.")

def get_redis_from_vcap() -> Redis:
    vcap_services = os.getenv("VCAP_SERVICES")
    if not vcap_services:
        raise RuntimeError("VCAP_SERVICES not found â€” are you running in CF?")
    
    services = json.loads(vcap_services)
    redis_creds = None

    # CF Redis might use 'redis' or another name depending on broker
    for service_type in services:
        if "redis" in service_type.lower():
            redis_creds = services[service_type][0]["credentials"]
            break

    if not redis_creds:
        raise RuntimeError("Redis credentials not found in VCAP_SERVICES.")

    return Redis.from_url(redis_creds["uri"])

redis_conn = get_redis_from_vcap()
queue = Queue("autoflow", connection=redis_conn)

app = FastAPI()

GITHUB_APP_TOKEN: str = os.getenv("GITHUB_APP_TOKEN")
BASE_REPO_DIR: str = "/tmp/repos"
PR_TITLE_TEMPLATE: str = "Auto generated Deployment files for {repo_name} by Autoflow"

repo_manager = RepositoryManager(BASE_REPO_DIR)
github_handler = GithubHandler(GITHUB_APP_TOKEN)
infra_generator = InfraGenerator("gemini-2.0-flash")

app_data = github_handler.get_app_data()

class AutoflowRequest(BaseModel):
    jobId: str
    repo_url: str
    repo_owner: str
    workflow_type: str
    required_files: str
    additional_requirements: str = ""
    branch: Optional[str] = "main"


def push_result(job_id: str, status: str, message: str):
    payload = json.dumps({
        "jobId": job_id,
        "status": status,
        "message": message
    })
    redis_conn.rpush("autoflow:results", payload)

def process_autflow_job(
        job_id: str, 
        repo_url: str, 
        repo_owner: str, 
        repo_branch: str, 
        workflow_type: str, 
        required_files: str, 
        additional_requirements: str = ""
    ):
    import traceback

    branch_name: str = f"autoflow-{repo_owner}-{uuid.uuid4().hex[:8]}"
    repo_name: str = repo_url.split("/")[-1].replace(".git", "")

    try:
        print(f"Generating {required_files}... ")
        print(f"Branch name: {branch_name}")
        print(f"Repo name: {repo_name}")
        authenticated_repo_url = github_handler.clone_repo_with_github_app(repo_url)
        repo_path, repo = repo_manager.clone_repo(authenticated_repo_url, branch_name, repo_branch)
        repo_metadata = github_handler.get_repo_metadata(repo_owner, repo_name)
        gh_repo = repo_metadata.pop("repo_obj")

        files: Dict[str, str] = infra_generator.generate_files(
            repo_metadata,
            workflow_type,
            required_files,
            additional_requirements
        )

        repo_manager.write_files(repo_path, files)
        repo_manager.commit_and_push(repo, branch_name, app_data, required_files)

        pr = github_handler.create_pull_request(
            gh_repo,
            branch_name,
            PR_TITLE_TEMPLATE.format(repo_name=repo_name),
            "This PR is auto-generated by Autoflow. Please review and merge."
        )
        print(f"[{job_id}] Success: PR created at {pr.html_url}")
        push_result(job_id, "success", f"Files generated, PR is available at: {pr.html_url}")
    except Exception as e:
        print(f"[{job_id}] Error: {str(e)}")
        traceback.print_exc()
    finally:
        print(f"[{job_id}] Cleaning up repo at {repo_path}")
        repo_manager.cleanup(repo_path)



@app.post("/")
async def autoflow_endpoint(request: AutoflowRequest):
    data = request.model_dump()
    job_id: str = data.get("jobId")
    repo_url: str = data.get("repo_url")
    repo_owner: str = data.get("repo_owner")
    workflow_type: str = data.get("workflow_type")
    required_files: str = data.get("required_files")
    additional_requirements = data.get("additional_requirements")
    repo_branch: str = data.get("branch")

    queue.enqueue(
        process_autflow_job,
        job_id,
        repo_url,
        repo_owner,
        repo_branch,
        workflow_type,
        required_files,
        additional_requirements,
        job_id=job_id
    )

    return {
        "jobId": job_id,
        "status": "queued",
        "message": "Job has been queued for processing."
    }